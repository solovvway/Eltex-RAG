import requests as r
from bs4 import BeautifulSoup
import os
import trafilatura
import re
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import numpy as np

docurl = 'https://docs.eltex-co.ru/ede/esr-series-user-manual-firmware-version-1-18-1-380863447.html'
baseurl = 'https://docs.eltex-co.ru'

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Qdrant
QDRANT_HOST = "localhost"
QDRANT_PORT = 6333
COLLECTION_NAME = "eltex_docs"

def getlinks(url):
    res = r.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    links = soup.find_all('a', class_=['accordion-title', 'toggle'])
    res = []
    for link in links:
        if 'href' in link.attrs:
            res.append(link['href'])
    return res

def getdocs(urls):
    for path in urls:
        full_url = baseurl + path
        try:
            res = r.get(full_url)
            filename = path[5:].split('-')[0] + ".html"
            with open(os.path.join('eltex_docs', filename), 'w', encoding='utf-8') as f:
                f.write(res.text)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {full_url}: {e}")

def getfilenames():
    files = os.listdir("eltex_docs")
    return [f for f in files if f.endswith(".html")]

def normalize(files):
    for filename in files:
        filepath = os.path.join("eltex_docs", filename)
        with open(filepath, "r", encoding="utf-8") as f:
            html = f.read()

        text = trafilatura.extract(html, include_comments=False, include_tables=False)

        if text:
            outpath = os.path.join("norm_docs", filename.replace(".html", ".txt"))
            with open(outpath, "w", encoding="utf-8") as f:
                f.write(text)
            print(f"‚úÖ {outpath}")
        else:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å: {filename}")

def create_chunks(files):
    all_chunks = []
    for file in files:
        chunks = []
        current_chunk = []
        filepath = os.path.join("norm_docs", file.replace(".html", ".txt"))
        if not os.path.exists(filepath):
            continue
        with open(filepath, 'r', encoding="utf-8") as text:
            lines = text.readlines()

        for line in lines:
            stripped = line.strip()
            if not stripped:
                continue

            current_chunk.append(stripped)

            if not stripped.startswith("esr"):
                if current_chunk and any(l.startswith("esr") for l in current_chunk[:-1]):
                    chunk_text = "\n".join(current_chunk).strip()
                    if chunk_text:
                        chunks.append(chunk_text)
                    current_chunk = []

        if current_chunk:
            chunk_text = "\n".join(current_chunk).strip()
            if chunk_text:
                chunks.append(chunk_text)

        all_chunks.extend(chunks)
    return all_chunks

def build_vectorstore(chunks, model_name="sentence-transformers/all-MiniLM-L6-v2"):
    model = SentenceTransformer(model_name)
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

    if not client.collection_exists(COLLECTION_NAME):
        dim = model.get_sentence_embedding_dimension()
        from qdrant_client.models import TextIndexParams, TokenizerType

        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=dim, distance=Distance.COSINE),
            # –í–∫–ª—é—á–∞–µ–º full-text –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–ª—è "text"
            hnsw_config=None,  # –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        )

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è BM25
        client.create_payload_index(
            collection_name=COLLECTION_NAME,
            field_name="text",
            field_schema=TextIndexParams(
                type="text",
                tokenizer=TokenizerType.WORD,  # –∏–ª–∏ "whitespace", "multilingual"
                min_token_len=2,
                max_token_len=15,
                lowercase=True
            )
        )

        # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –∏–Ω–¥–µ–∫—Å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω
        indexes = client.get_collection(COLLECTION_NAME).payload_schema
        print("üìë –ò–Ω–¥–µ–∫—Å—ã –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:")
        for field_name, schema in indexes.items():
            print(f" - {field_name}: {schema}")


        print(f"üÜï –ö–æ–ª–ª–µ–∫—Ü–∏—è {COLLECTION_NAME} —Å–æ–∑–¥–∞–Ω–∞ —Å BM25-–∏–Ω–¥–µ–∫—Å–æ–º")

        embeddings = model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)
        points = [
            PointStruct(id=i, vector=emb.tolist(), payload={"text": chunk})
            for i, (chunk, emb) in enumerate(zip(chunks, embeddings))
        ]
        client.upsert(collection_name=COLLECTION_NAME, points=points)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    else:
        print(f"üìÇ –ö–æ–ª–ª–µ–∫—Ü–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

    return model, client

import requests

def query_bm25(question, client=None, top_k=5):
    try:
        resp = requests.post(
            f"http://localhost:6333/collections/{COLLECTION_NAME}/points/query",
            json={
                "limit": top_k,
                "query": {"text": question},
                "using": "text"
            }
        )

        resp.raise_for_status()
        points = resp.json()["result"]["points"]
        return [(p["payload"]["text"], "BM25") for p in points]
    except Exception as e:
        print(f"‚ö†Ô∏è BM25 search error: {e}")
        return []

def query_vector(question, model, client, top_k=5):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π: (text, 'Vector')"""
    try:
        q_emb = model.encode([question], convert_to_numpy=True)[0].tolist()
        search_result = client.query_points(
            collection_name=COLLECTION_NAME,
            query=q_emb,  # –≤–µ–∫—Ç–æ—Ä –ø–µ—Ä–µ–¥–∞—ë–º –∫–∞–∫ query
            using=None,   # None = –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –≤–µ–∫—Ç–æ—Ä
            limit=top_k
        )
        return [(hit.payload["text"], "Vector") for hit in search_result.points]
    except Exception as e:
        print(f"‚ö†Ô∏è Vector search error: {e}")
        return []


def hybrid_query(question, model, client, bm25_k=5, vector_k=5, total_k=10):
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: BM25 + Vector.
    –£–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç BM25.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (text, source)
    """
    bm25_results = query_bm25(question, client, bm25_k)
    vector_results = query_vector(question, model, client, vector_k)

    seen = set()
    hybrid = []

    # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º BM25 (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç!)
    for text, src in bm25_results:
        if text not in seen:
            hybrid.append((text, src))
            seen.add(text)

    # –ü–æ—Ç–æ–º ‚Äî –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã—Ö –µ—â—ë –Ω–µ—Ç
    for text, src in vector_results:
        if text not in seen and len(hybrid) < total_k:
            hybrid.append((text, src))
            seen.add(text)

    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ total_k
    return hybrid[:total_k]


def query_rag(question, model, client, top_k=10):
    q_emb = model.encode([question], convert_to_numpy=True)[0].tolist()
    search_result = client.search(
        collection_name=COLLECTION_NAME,
        query_vector=q_emb,
        limit=top_k
    )
    results = [hit.payload["text"] for hit in search_result]
    return results

def rag_answer(question, model, client, top_k=6, apiurl="http://localhost:8080/v1/chat/completions"):
    hybrid_results = hybrid_query(
        question=question,
        model=model,
        client=client,
        bm25_k=3,
        vector_k=3,
        total_k=top_k
    )

    context_lines = []
    for i, (text, source) in enumerate(hybrid_results, 1):
        context_lines.append(f"[{source}] Chunk #{i}:\n{text}\n{'-'*50}")

    context = "\n\n".join(context_lines)
    with open("prompt.txt", "w") as file:
        file.write(context)

    prompt = f"""
You are a helpful assistant for network engineer that work with Eltex routers. 
Answer the question using the context below.
If answer is not found, say 'not enough data'.
Answer briefly and to the point. Answer questions primarily using configuration commands, writing them down in ```.

For example, for the question ‚ÄòHow do I configure the access port on the switch?‚Äô, give the answer

Context:
{context}

Question: {question}
"""

    payload = {
        "model": "qwen3",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant working with vendor documentation."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.0,
        "max_tokens": 512
    }

    # resp = r.post(apiurl, json=payload)
    # resp.raise_for_status()
    # data = resp.json()
    return prompt


# from qdrant_client import QdrantClient
import csv

client = QdrantClient(host="localhost", port=6333)
collection_name = "eltex_docs"

# –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ç–æ—á–∫–∏ (–ø–æ —á–∞—Å—Ç—è–º, –µ—Å–ª–∏ –∏—Ö –º–Ω–æ–≥–æ)
all_points = []
offset = None
while True:
    records, offset = client.scroll(
        collection_name=collection_name,
        limit=1000,  # –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å
        offset=offset,
        with_payload=True,
        with_vectors=False  # –Ω–µ –≤—ã–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã, —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
    )
    all_points.extend(records)
    if offset is None:
        break

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
with open("qdrant_export.csv", "w", encoding="utf-8", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(["id", "text"])
    for point in all_points:
        writer.writerow([point.id, point.payload.get("text", "")])

print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(all_points)} –∑–∞–ø–∏—Å–µ–π –≤ qdrant_export.csv")


# === –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ç–æ–∫ ===
# input_dir = "eltex_docs"
# output_dir = "norm_docs"
# os.makedirs(input_dir, exist_ok=True)
# os.makedirs(output_dir, exist_ok=True)

# client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
# client.delete_collection("eltex_docs")

# # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ:
# links = getlinks(docurl)
# getdocs(links)
# files = getfilenames()
# normalize(files)

# files = getfilenames()
# chunks = create_chunks(files)
# print("–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞:", chunks[1] if chunks else "–ù–µ—Ç —á–∞–Ω–∫–æ–≤")

# model, client = build_vectorstore(chunks)


# model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
# client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
# # question = input(">")
# question = 'configure mpls l2vpn in compella mode'
# results = rag_answer(question, model, client)
# print(results)