import os
import requests as r
from bs4 import BeautifulSoup
import trafilatura
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, TextIndexParams, TokenizerType
from openai import OpenAI
from dotenv import load_dotenv
load_dotenv()  # <-- Ð²Ð°Ð¶Ð½Ð¾: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ .env Ð² os.environ

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
YANDEX_CLOUD_API_KEY = os.getenv("YANDEX_CLOUD_API_KEY")
YANDEX_CLOUD_FOLDER = os.getenv("YANDEX_CLOUD_FOLDER")

# Ð’Ñ‹Ð±Ð¾Ñ€ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð° (Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸Ð·Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð½Ð° "yandex" Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Yandex Cloud)
PROVIDER = "openrouter"  # Ð¸Ð»Ð¸ "yandex"

if PROVIDER == "openrouter":
    if not OPENROUTER_API_KEY:
        raise RuntimeError("âŒ Ð£ÐºÐ°Ð¶Ð¸ OPENROUTER_API_KEY Ð² .env")
    llm_client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=OPENROUTER_API_KEY,
    )
elif PROVIDER == "yandex":
    if not YANDEX_CLOUD_API_KEY:
        raise RuntimeError("âŒ Ð£ÐºÐ°Ð¶Ð¸ YANDEX_CLOUD_API_KEY Ð² .env")
    if not YANDEX_CLOUD_FOLDER:
        raise RuntimeError("âŒ Ð£ÐºÐ°Ð¶Ð¸ YANDEX_CLOUD_FOLDER Ð² .env Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Yandex Cloud")
    llm_client = OpenAI(
        api_key=YANDEX_CLOUD_API_KEY,
        base_url="https://llm.api.cloud.yandex.net/v1",
        project=YANDEX_CLOUD_FOLDER
    )
else:
    raise RuntimeError("âŒ ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ 'openrouter' Ð¸Ð»Ð¸ 'yandex'")
# URLs
doccli = 'https://docs.eltex-co.ru/ede/esr-series-user-manual-firmware-version-1-18-1-380863447.html'
docguide = 'https://docs.eltex-co.ru/ede/esr-series-cli-command-reference-guide-firmware-version-1-13-0-177668705.html'
baseurl = 'https://docs.eltex-co.ru'

# Qdrant settings
QDRANT_HOST = "localhost"
QDRANT_PORT = 6333
COLLECTION_NAME = "eltex_docs"
# ÐœÐ¾Ð´ÐµÐ»Ð¸ Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð°
if PROVIDER == "openrouter":
    models = [
        "google/gemma-3n-e2b-it:free",
        "deepseek/deepseek-chat-v3.1:free",
        "openai/gpt-oss-20b:free",
        "qwen/qwen3-coder:free",
        "deepseek/deepseek-r1-distill-llama-70b:free"
    ]
else:  # yandex
    models = [
        f"gpt://{YANDEX_CLOUD_FOLDER}/yandexgpt/latest",
        f"gpt://{YANDEX_CLOUD_FOLDER}/yandexgpt-lite/latest"
    ]

input_dir = "eltex_docs"
os.makedirs(input_dir, exist_ok=True)

# === Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¸ ===

def getlinks(url):
    res = r.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    links = soup.find_all('a', class_=['accordion-title', 'toggle'])
    return [link['href'] for link in links if 'href' in link.attrs]

def download_docs_if_needed():
    if os.listdir(input_dir):
        print("ðŸ“ Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ ÑƒÐ¶Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ðµ.")
        return

    print("ðŸ“¥ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸...")
    for doc_url in [doccli, docguide]:
        links = getlinks(doc_url)
        for path in links:
            full_url = baseurl + path
            try:
                res = r.get(full_url)
                filename = path.lstrip('/').replace('/', '_').replace('.html', '') + ".html"
                with open(os.path.join(input_dir, filename), 'w', encoding='utf-8') as f:
                    f.write(res.text)
                print(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾: {filename}")
            except Exception as e:
                print(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐµ {full_url}: {e}")

def extract_chunks_from_html(html_content, source_file):
    soup = BeautifulSoup(html_content, "html.parser")
    main = soup.find("div", {"id": "main-content"}) or soup

    # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ h2, ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ â€” h1
    headings = main.find_all(["h2", "h1"])
    if not headings:
        # fallback: Ð²ÐµÑÑŒ Ñ‚ÐµÐºÑÑ‚ ÐºÐ°Ðº Ð¾Ð´Ð¸Ð½ Ñ‡Ð°Ð½Ðº
        text = trafilatura.extract(html_content, include_comments=False, include_tables=False)
        if text and len(text.strip()) > 20:
            return [{"source_file": source_file, "title": "Document", "text": text.strip()}]
        return []

    chunks = []
    for i, h in enumerate(headings):
        title = h.get_text(strip=True)
        if not title:
            continue

        content_parts = []
        sibling = h.next_sibling
        while sibling:
            if sibling.name in ["h1", "h2"]:
                break
            if hasattr(sibling, 'get_text'):
                txt = sibling.get_text(strip=True)
                if txt:
                    content_parts.append(txt)
            elif isinstance(sibling, str):
                txt = sibling.strip()
                if txt:
                    content_parts.append(txt)
            sibling = sibling.next_sibling

        chunk_text = "\n".join(content_parts).strip()
        full_text = f"{title}\n\n{chunk_text}".strip()
        if len(full_text) < 20:
            continue

        chunks.append({
            "source_file": source_file,
            "title": title,
            "text": full_text
        })
    return chunks

def load_all_chunks():
    all_chunks = []
    for filename in os.listdir(input_dir):
        if not filename.endswith(".html"):
            continue
        filepath = os.path.join(input_dir, filename)
        with open(filepath, "r", encoding="utf-8") as f:
            html = f.read()
        chunks = extract_chunks_from_html(html, filename)
        all_chunks.extend(chunks)
        print(f"ðŸ“„ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾: {filename} â†’ {len(chunks)} Ñ‡Ð°Ð½ÐºÐ¾Ð²")
    return [ch["text"] for ch in all_chunks]  # Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚ÐµÐºÑÑ‚ Ð´Ð»Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²

def ensure_collection_exists(client, model, chunks):
    if client.collection_exists(COLLECTION_NAME):
        print(f"ðŸ“‚ ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ {COLLECTION_NAME} ÑƒÐ¶Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚.")
        return
    # client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
    # client.delete_collection("eltex_docs")
    print(f"ðŸ†• Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ {COLLECTION_NAME}...")
    dim = model.get_sentence_embedding_dimension()
    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(size=dim, distance=Distance.COSINE)
    )
    client.create_payload_index(
        collection_name=COLLECTION_NAME,
        field_name="text",
        field_schema=TextIndexParams(
            type="text",
            tokenizer=TokenizerType.WORD,
            min_token_len=2,
            max_token_len=15,
            lowercase=True
        )
    )

    print("ðŸ§  Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²...")
    embeddings = model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)
    points = [
        PointStruct(id=i, vector=emb.tolist(), payload={"text": chunk})
        for i, (chunk, emb) in enumerate(zip(chunks, embeddings))
    ]
    client.upsert(collection_name=COLLECTION_NAME, points=points)
    print(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(chunks)} Ñ‡Ð°Ð½ÐºÐ¾Ð² Ð² Qdrant.")
    # --- Ð“Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº (BM25 + Vector) ---
def query_bm25(question, top_k=5):
    try:
        resp = r.post(
            f"http://localhost:6333/collections/{COLLECTION_NAME}/points/query",
            json={"limit": top_k, "query": {"text": question}, "using": "text"}
        )
        resp.raise_for_status()
        return [p["payload"]["text"] for p in resp.json()["result"]["points"]]
    except Exception as e:
        print(f"âš ï¸ BM25 error: {e}")
        return []

def query_vector(question, top_k=5):
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

    q_emb = model.encode([question])[0].tolist()
    hits = client.query_points(collection_name=COLLECTION_NAME, query=q_emb, limit=top_k).points
    return [hit.payload["text"] for hit in hits]

def chat_with_model(model_name: str, prompt: str, site_url: str = "", site_name: str = ""):
    headers = {}
    if site_url:
        headers["HTTP-Referer"] = site_url
    if site_name:
        headers["X-Title"] = site_name

    completion = llm_client.chat.completions.create(
        extra_headers=headers,
        model=model_name,
        messages=[{"role": "user", "content": prompt}]
    )
    response = completion.choices[0].message.content
    print(response)
    return response
def get_key_info():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ ÐºÐ»ÑŽÑ‡Ðµ: Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ðº, Ð»Ð¸Ð¼Ð¸Ñ‚, Ð¸ Ð´Ñ€. (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ OpenRouter)"""
    if PROVIDER != "openrouter" or not OPENROUTER_API_KEY:
        return {}
    try:
        resp = r.get("https://openrouter.ai/api/v1/key",
                     headers={"Authorization": f"Bearer {OPENROUTER_API_KEY}"})
        resp.raise_for_status()
        return resp.json()["data"]
    except Exception:
        return {}

def can_use_free_model():
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð±ÐµÑÐ¿Ð»Ð°Ñ‚Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ OpenRouter)"""
    if PROVIDER != "openrouter":
        return True  # Ð”Ð»Ñ Yandex Cloud Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ
    info = get_key_info()
    # info ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚: limit, limit_remaining, usage_daily Ð¸ Ð´Ñ€.
    # is_free_tier ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, Ð±Ñ‹Ð» Ð»Ð¸ Ñ€Ð°Ð½ÐµÐµ ÐºÑƒÐ¿Ð»ÐµÐ½ Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ ÐºÑ€ÐµÐ´Ð¸Ñ‚Ð¾Ð²
    return info.get("is_free_tier", False)

def chat_with_model_auto(model_name: str, prompt: str, site_url: str = "", site_name: str = ""):
    # Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ»ÑŽÑ‡
    key_info = get_key_info()
    rem = key_info.get("limit_remaining")
    # Ð•ÑÐ»Ð¸ Ð½ÐµÑ‚ Ð»Ð¸Ð¼Ð¸Ñ‚Ð° Ð¸Ð»Ð¸ Ð¾Ð½ Ð½ÑƒÐ»ÐµÐ²Ð¾Ð¹ â€” Ð½ÐµÐ»ÑŒÐ·Ñ
    if rem is not None and rem <= 0:
        raise RuntimeError("ÐšÐ²Ð¾Ñ‚Ð° Ð½Ð° ÐºÐ»ÑŽÑ‡ Ð¸ÑÑ‡ÐµÑ€Ð¿Ð°Ð½Ð°")

    # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾Ñ
    try:
        resp = llm_client.chat.completions.create(
            extra_headers={**({"HTTP-Referer": site_url} if site_url else {}),
                           **({"X-Title": site_name} if site_name else {})},
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            # ÐœÐ¾Ð¶Ð½Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¸Ñ‚ÑŒ usage Ð²Ð¼ÐµÑÑ‚Ðµ Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð¼, ÐµÑÐ»Ð¸ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐ°Ð½Ð¾ Ð² API
            usage={"include": True}
        )
    except HTTPError as e:
        # Ð•ÑÐ»Ð¸ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐµÐ½ â€” Ð¾ÑˆÐ¸Ð±ÐºÐ° 429, Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡Ð°Ñ‚ÑŒÑÑ
        if e.response.status_code == 429:
            # ÐœÐ¾Ð¶Ð½Ð¾ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð»Ð¸Ð¼Ð¸Ñ‚Ð° Ð² Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°Ñ… Ð¸Ð»Ð¸ Ñ‚ÐµÐ»Ðµ
            # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ñ€ÑƒÐ³ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
            print("âš ï¸ Rate limit reached for model", model_name)
            return None, "rate_limit"
        else:
            raise

    result = resp.choices[0].message.content
    usage = getattr(resp, "usage", None)
    return result, usage

# ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð»Ð¾Ð³Ð¸ÐºÐ¸ Ð²Ñ‹Ð±Ð¾Ñ€Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸
def ask(prompt):
    # Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿Ð¾ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚Ñƒ
    for m in models:
        ans, usage_or_flag = chat_with_model_auto(m, prompt, site_url="", site_name="")
        if ans is not None:
            return ans
        # ÐµÑÐ»Ð¸ rate_limit, Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð´Ð°Ð»ÑŒÑˆÐµ
    # ÐµÑÐ»Ð¸ Ð½Ð¸ Ð¾Ð´Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð²ÐµÑ€Ð½ÑƒÐ»Ð° â€” Ð¾ÑˆÐ¸Ð±ÐºÐ°
    raise RuntimeError("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð½Ð°Ð¹Ñ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ð¼ Ð»Ð¸Ð¼Ð¸Ñ‚Ð¾Ð¼")
if __name__ == "__main__":
    # 1. Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÐ¼, ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
    download_docs_if_needed()

    # 2. Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¸ Ñ‡Ð°Ð½ÐºÑƒÐµÐ¼
    chunks = load_all_chunks()
    if not chunks:
        raise RuntimeError("âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð½Ð¸ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ñ‡Ð°Ð½ÐºÐ°.")

    # 3. Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ Qdrant
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

    # 4. Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ, ÐµÑÐ»Ð¸ ÐµÑ‘ Ð½ÐµÑ‚
    ensure_collection_exists(client, model, chunks)

    # 5. ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
    question = 'create l3vpn with on vrfs TEST1 with rd 1234:123 on router 1.1.1.1 and router 2.2.2.2. Give me 2 configs to this routers'

    bm25_res = query_bm25(question, 3)
    vector_res = query_vector(question, 3)

    seen = set()
    hybrid = []
    for t in bm25_res:
        if t not in seen:
            hybrid.append(t); seen.add(t)
    for t in vector_res:
        if t not in seen and len(hybrid) < 6:
            hybrid.append(t); seen.add(t)

    context = "\n\n---\n\n".join(hybrid)

    prompt = f"""
You are a helpful assistant for network engineer that work with Eltex routers. 
Answer the question using the context below.
If answer is not found, say 'not enough data'.
Answer briefly and to the point. Answer questions primarily using configuration commands, writing them down in ```.
Question: {question}

Context:
{context}
"""

    with open("prompt.txt", "w", encoding="utf-8") as f:
        f.write(prompt)

    print("âœ… ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½ Ð² prompt.txt")
    output = chat_with_model(models[2],prompt)
    print(output)