import os
import re
import logging
import requests
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    ApplicationBuilder, CommandHandler, MessageHandler,
    CallbackQueryHandler, ContextTypes, filters
)
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, TextIndexParams, TokenizerType
from openai import OpenAI
from requests.exceptions import HTTPError

# === ENV ===
load_dotenv()
API_KEY = os.getenv("OPENROUTER_API_KEY")
BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")

if not API_KEY or not BOT_TOKEN:
    raise RuntimeError("‚ùå –£–∫–∞–∂–∏ OPENROUTER_API_KEY –∏ TELEGRAM_BOT_TOKEN –≤ .env")

# === OpenRouter client ===
llm_client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=API_KEY)

# === Qdrant ===
QDRANT_HOST = "localhost"
QDRANT_PORT = 6333
COLLECTION_NAME = "eltex_docs"

# === –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ===
user_model_choice = {}  # {user_id: model_name}
cached_free_models = []


# === –ü–æ–ª—É—á–µ–Ω–∏–µ –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π ===
def get_free_models():
    """–ü–æ–ª—É—á–∏—Ç—å –∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É."""
    global cached_free_models
    if cached_free_models:
        return cached_free_models

    resp = requests.get(
        "https://openrouter.ai/api/v1/models",
        headers={"Authorization": f"Bearer {API_KEY}"}
    )
    resp.raise_for_status()
    data = resp.json().get("data", [])

    free_models = [m["id"] for m in data if m.get("id", "").endswith(":free")]
    # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–∞–∑–º–µ—Ä—É –º–æ–¥–µ–ª–∏ (70b > 33b > 20b > 7b > 3b > 1b)
    def extract_size(model_id):
        match = re.search(r"(\d+)(b|B)", model_id)
        return int(match.group(1)) if match else 0

    free_models.sort(key=extract_size, reverse=True)
    cached_free_models = free_models
    return free_models


# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–≤–æ—Ç—ã / –∫–ª—é—á–∞ ===
def get_key_info():
    resp = requests.get("https://openrouter.ai/api/v1/key", headers={"Authorization": f"Bearer {API_KEY}"})
    resp.raise_for_status()
    return resp.json().get("data", {})


# === –†–∞–±–æ—Ç–∞ —Å Qdrant ===
def query_bm25(question, top_k=5):
    try:
        resp = requests.post(
            f"http://localhost:6333/collections/{COLLECTION_NAME}/points/query",
            json={"limit": top_k, "query": {"text": question}, "using": "text"}
        )
        resp.raise_for_status()
        return [p["payload"]["text"] for p in resp.json()["result"]["points"]]
    except Exception:
        return []


def query_vector(question, top_k=5):
    emb_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
    q_emb = emb_model.encode([question])[0].tolist()
    hits = client.query_points(collection_name=COLLECTION_NAME, query=q_emb, limit=top_k).points
    return [h.payload["text"] for h in hits]


# === –†–∞–±–æ—Ç–∞ —Å –º–æ–¥–µ–ª—è–º–∏ ===
def chat_with_model_safe(model_name: str, prompt: str):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –∏–ª–∏ None –ø—Ä–∏ rate_limit."""
    try:
        resp = llm_client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}]
        )
        return resp.choices[0].message.content
    except HTTPError as e:
        if e.response.status_code == 429:
            return None  # –∫–≤–æ—Ç–∞ –∏—Å—á–µ—Ä–ø–∞–Ω–∞
        raise


async def ask_with_fallback(update: Update, model_name: str, prompt: str):
    """–ü—Ä–æ–±—É–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –ø—Ä–∏ –ª–∏–º–∏—Ç–µ ‚Äî –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é."""
    free_models = get_free_models()

    if model_name not in free_models:
        await update.message.reply_text(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_name} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.")
        model_name = free_models[0]
        user_model_choice[update.message.from_user.id] = model_name

    idx = free_models.index(model_name)
    for i in range(idx, len(free_models)):
        current_model = free_models[i]
        response = chat_with_model_safe(current_model, prompt)
        print(prompt,'\n',response,'\n')
        if response:
            if i != idx:
                await update.message.reply_text(
                    f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_name} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (–ª–∏–º–∏—Ç), –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ {current_model}"
                )
                user_model_choice[update.message.from_user.id] = current_model
            return response
    return "‚ùå –í—Å–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏—Å—á–µ—Ä–ø–∞–ª–∏ –ª–∏–º–∏—Ç. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ."


# === Telegram Bot ===
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    free_models = get_free_models()
    keyboard = [[InlineKeyboardButton(m, callback_data=m)] for m in free_models]
    reply_markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text("üëã –ü—Ä–∏–≤–µ—Ç! –í—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã:", reply_markup=reply_markup)


async def select_model(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    model = query.data
    user_model_choice[query.from_user.id] = model
    await query.edit_message_text(f"‚úÖ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model}\n–¢–µ–ø–µ—Ä—å –æ—Ç–ø—Ä–∞–≤—å –≤–æ–ø—Ä–æ—Å.")


async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    model = user_model_choice.get(user_id)

    if not model:
        await update.message.reply_text("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é /start")
        return

    question = update.message.text.strip()
    await update.message.reply_text("üîé –ò—â—É –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏...")

    bm25_res = query_bm25(question, 3)
    vector_res = query_vector(question, 3)

    hybrid = []
    seen = set()
    for t in bm25_res + vector_res:
        if t not in seen:
            hybrid.append(t)
            seen.add(t)
    context_text = "\n\n---\n\n".join(hybrid)

    prompt = f"""
You are a helpful assistant for network engineer that work with Eltex routers. 
Answer the question using the context below.
If answer is not found, say 'not enough data'.
Answer briefly and to the point. Use configuration commands.
Question: {question}

Context:
{context_text}
"""

    await update.message.reply_text("üß† –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –º–æ–¥–µ–ª–∏...")
    response = await ask_with_fallback(update, model, prompt)
    await update.message.reply_text(response)


def main():
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CallbackQueryHandler(select_model))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω.")
    app.run_polling()


if __name__ == "__main__":
    main()
