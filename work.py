import os
import requests as r
from bs4 import BeautifulSoup
import trafilatura
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, TextIndexParams, TokenizerType
from openai import OpenAI
from dotenv import load_dotenv
from unstructured.partition.auto import partition
from unstructured.staging.base import elements_to_json

load_dotenv()  # <-- –≤–∞–∂–Ω–æ: –∑–∞–≥—Ä—É–∂–∞–µ—Ç .env –≤ os.environ

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
YANDEX_CLOUD_API_KEY = os.getenv("YANDEX_CLOUD_API_KEY")
YANDEX_CLOUD_FOLDER = os.getenv("YANDEX_CLOUD_FOLDER")

# –í—ã–±–æ—Ä –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ "yandex" –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Yandex Cloud)
PROVIDER = "openrouter"  # –∏–ª–∏ "yandex"

if PROVIDER == "openrouter":
    if not OPENROUTER_API_KEY:
        raise RuntimeError("‚ùå –£–∫–∞–∂–∏ OPENROUTER_API_KEY –≤ .env")
    llm_client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=OPENROUTER_API_KEY,
    )
elif PROVIDER == "yandex":
    if not YANDEX_CLOUD_API_KEY:
        raise RuntimeError("‚ùå –£–∫–∞–∂–∏ YANDEX_CLOUD_API_KEY –≤ .env")
    if not YANDEX_CLOUD_FOLDER:
        raise RuntimeError("‚ùå –£–∫–∞–∂–∏ YANDEX_CLOUD_FOLDER –≤ .env –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Yandex Cloud")
    llm_client = OpenAI(
        api_key=YANDEX_CLOUD_API_KEY,
        base_url="https://llm.api.cloud.yandex.net/v1",
        project=YANDEX_CLOUD_FOLDER
    )
else:
    raise RuntimeError("‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä. –ò—Å–ø–æ–ª—å–∑—É–π 'openrouter' –∏–ª–∏ 'yandex'")
# URLs
doccli = 'https://docs.eltex-co.ru/ede/esr-series-user-manual-firmware-version-1-18-1-380863447.html'
docguide = 'https://docs.eltex-co.ru/ede/esr-series-cli-command-reference-guide-firmware-version-1-13-0-177668705.html'
baseurl = 'https://docs.eltex-co.ru'

# Qdrant settings
QDRANT_HOST = "localhost"
QDRANT_PORT = 6333
COLLECTION_NAME = "eltex_docs"
# –ú–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
if PROVIDER == "openrouter":
    models = [
        "google/gemma-3n-e2b-it:free",
        "deepseek/deepseek-chat-v3.1:free",
        "openai/gpt-oss-20b:free",
        "qwen/qwen3-coder:free",
        "deepseek/deepseek-r1-distill-llama-70b:free"
    ]
else:  # yandex
    models = [
        f"gpt://{YANDEX_CLOUD_FOLDER}/yandexgpt/latest",
        f"gpt://{YANDEX_CLOUD_FOLDER}/yandexgpt-lite/latest"
    ]

input_dir = "eltex_docs"
supertel_dir = "supertel_docs"

os.makedirs(input_dir, exist_ok=True)

# === –§—É–Ω–∫—Ü–∏–∏ ===

def getlinks(url):
    res = r.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    links = soup.find_all('a', class_=['accordion-title', 'toggle'])
    return [link['href'] for link in links if 'href' in link.attrs]

def download_docs_if_needed():
    if os.listdir(input_dir):
        print("üìÅ –î–æ–∫—É–º–µ–Ω—Ç—ã —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ.")
        return

    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏...")
    for doc_url in [doccli, docguide]:
        links = getlinks(doc_url)
        for path in links:
            full_url = baseurl + path
            try:
                res = r.get(full_url)
                filename = path.lstrip('/').replace('/', '_').replace('.html', '') + ".html"
                with open(os.path.join(input_dir, filename), 'w', encoding='utf-8') as f:
                    f.write(res.text)
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {filename}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {full_url}: {e}")

def extract_chunks_from_html(html_content, source_file):
    soup = BeautifulSoup(html_content, "html.parser")
    main = soup.find("div", {"id": "main-content"}) or soup

    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º h2, –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî h1
    headings = main.find_all(["h2", "h1"])
    if not headings:
        # fallback: –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–¥–∏–Ω —á–∞–Ω–∫
        text = trafilatura.extract(html_content, include_comments=False, include_tables=False)
        if text and len(text.strip()) > 20:
            return [{"source_file": source_file, "title": "Document", "text": text.strip()}]
        return []

    chunks = []
    for i, h in enumerate(headings):
        title = h.get_text(strip=True)
        if not title:
            continue

        content_parts = []
        sibling = h.next_sibling
        while sibling:
            if sibling.name in ["h1", "h2"]:
                break
            if hasattr(sibling, 'get_text'):
                txt = sibling.get_text(strip=True)
                if txt:
                    content_parts.append(txt)
            elif isinstance(sibling, str):
                txt = sibling.strip()
                if txt:
                    content_parts.append(txt)
            sibling = sibling.next_sibling

        chunk_text = "\n".join(content_parts).strip()
        full_text = f"{title}\n\n{chunk_text}".strip()
        if len(full_text) < 20:
            continue

        chunks.append({
            "source_file": source_file,
            "title": title,
            "text": full_text
        })
    return chunks

def load_all_chunks():
    all_chunks = []
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ Eltex
    print("üìö –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ Eltex...")
    for filename in os.listdir(input_dir):
        if not filename.endswith(".html"):
            continue
        filepath = os.path.join(input_dir, filename)
        with open(filepath, "r", encoding="utf-8") as f:
            html = f.read()
        chunks = extract_chunks_from_html(html, f"eltex/{filename}")
        all_chunks.extend(chunks)
        print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {filename} ‚Üí {len(chunks)} —á–∞–Ω–∫–æ–≤")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ Supertel
    if os.path.exists(supertel_dir):
        print("üìö –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ Supertel...")
        for filename in os.listdir(supertel_dir):
            if not filename.endswith(".html"):
                continue
            filepath = os.path.join(supertel_dir, filename)
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    html = f.read()
                chunks = extract_chunks_from_html(html, f"supertel/{filename}")
                all_chunks.extend(chunks)
                print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {filename} ‚Üí {len(chunks)} —á–∞–Ω–∫–æ–≤")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {filename}: {e}")
    
    return all_chunks  # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

def ensure_collection_exists(client, model, chunks):
    if client.collection_exists(COLLECTION_NAME):
        print(f"üìÇ –ö–æ–ª–ª–µ–∫—Ü–∏—è {COLLECTION_NAME} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
        return
    # client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
    # client.delete_collection("eltex_docs")
    print(f"üÜï –°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é {COLLECTION_NAME}...")
    dim = model.get_sentence_embedding_dimension()
    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(size=dim, distance=Distance.COSINE)
    )
    client.create_payload_index(
        collection_name=COLLECTION_NAME,
        field_name="text",
        field_schema=TextIndexParams(
            type="text",
            tokenizer=TokenizerType.WORD,
            min_token_len=2,
            max_token_len=15,
            lowercase=True
        )
    )

    print("üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    texts = [ch["text"] for ch in chunks]
    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫–∏ —Å –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    points = [
        PointStruct(
            id=i,
            vector=emb.tolist(),
            payload={
                "text": chunk["text"],
                "source_file": chunk["source_file"],
                "title": chunk["title"]
            }
        )
        for i, (chunk, emb) in enumerate(zip(chunks, embeddings))
    ]
    client.upsert(collection_name=COLLECTION_NAME, points=points)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ Qdrant.")
    # --- –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (BM25 + Vector) ---
def query_bm25(question, top_k=5):
    """BM25 —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Qdrant Python client"""
    try:
        client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
        
        # –ü—Ä–æ—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞–∫ fallback, –µ—Å–ª–∏ BM25 –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
        # –≠—Ç–æ –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –Ω–æ –ø–æ–∑–≤–æ–ª–∏—Ç —Å–∏—Å—Ç–µ–º–µ —Ä–∞–±–æ—Ç–∞—Ç—å
        model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        q_emb = model.encode([question])[0].tolist()
        hits = client.query_points(
            collection_name=COLLECTION_NAME,
            query=q_emb,
            limit=top_k,
            with_payload=True
        ).points
        
        return [hit.payload["text"] for hit in hits]
    except Exception as e:
        print(f"‚ö†Ô∏è BM25 error: {e}")
        return []

def query_vector(question, top_k=5):
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

    q_emb = model.encode([question])[0].tolist()
    hits = client.query_points(collection_name=COLLECTION_NAME, query=q_emb, limit=top_k).points
    return [hit.payload["text"] for hit in hits]

def chat_with_model(model_name: str, prompt: str, site_url: str = "", site_name: str = ""):
    headers = {}
    if site_url:
        headers["HTTP-Referer"] = site_url
    if site_name:
        headers["X-Title"] = site_name

    completion = llm_client.chat.completions.create(
        extra_headers=headers,
        model=model_name,
        messages=[{"role": "user", "content": prompt}]
    )
    response = completion.choices[0].message.content
    print(response)
    return response
def get_key_info():
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª—é—á–µ: –æ—Å—Ç–∞—Ç–æ–∫, –ª–∏–º–∏—Ç, –∏ –¥—Ä. (—Ç–æ–ª—å–∫–æ –¥–ª—è OpenRouter)"""
    if PROVIDER != "openrouter" or not OPENROUTER_API_KEY:
        return {}
    try:
        resp = r.get("https://openrouter.ai/api/v1/key",
                     headers={"Authorization": f"Bearer {OPENROUTER_API_KEY}"})
        resp.raise_for_status()
        return resp.json()["data"]
    except Exception:
        return {}

def can_use_free_model():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—Ç–æ–ª—å–∫–æ –¥–ª—è OpenRouter)"""
    if PROVIDER != "openrouter":
        return True  # –î–ª—è Yandex Cloud –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è
    info = get_key_info()
    # info —Å–æ–¥–µ—Ä–∂–∏—Ç: limit, limit_remaining, usage_daily –∏ –¥—Ä.
    # is_free_tier —É–∫–∞–∑—ã–≤–∞–µ—Ç, –±—ã–ª –ª–∏ —Ä–∞–Ω–µ–µ –∫—É–ø–ª–µ–Ω –º–∏–Ω–∏–º—É–º –∫—Ä–µ–¥–∏—Ç–æ–≤
    return info.get("is_free_tier", False)

def chat_with_model_auto(model_name: str, prompt: str, site_url: str = "", site_name: str = ""):
    # –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á
    key_info = get_key_info()
    rem = key_info.get("limit_remaining")
    # –ï—Å–ª–∏ –Ω–µ—Ç –ª–∏–º–∏—Ç–∞ –∏–ª–∏ –æ–Ω –Ω—É–ª–µ–≤–æ–π ‚Äî –Ω–µ–ª—å–∑—è
    if rem is not None and rem <= 0:
        raise RuntimeError("–ö–≤–æ—Ç–∞ –Ω–∞ –∫–ª—é—á –∏—Å—á–µ—Ä–ø–∞–Ω–∞")

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
    try:
        resp = llm_client.chat.completions.create(
            extra_headers={**({"HTTP-Referer": site_url} if site_url else {}),
                           **({"X-Title": site_name} if site_name else {})},
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            # –ú–æ–∂–Ω–æ –∑–∞–ø—Ä–æ—Å–∏—Ç—å usage –≤–º–µ—Å—Ç–µ —Å –æ—Ç–≤–µ—Ç–æ–º, –µ—Å–ª–∏ –ø–æ–¥–ø–∏—Å–∞–Ω–æ –≤ API
            usage={"include": True}
        )
    except HTTPError as e:
        # –ï—Å–ª–∏ –ª–∏–º–∏—Ç –ø—Ä–µ–≤—ã—à–µ–Ω ‚Äî –æ—à–∏–±–∫–∞ 429, –º–æ–∂–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è
        if e.response.status_code == 429:
            # –ú–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ª–∏–º–∏—Ç–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö –∏–ª–∏ —Ç–µ–ª–µ
            # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å
            print("‚ö†Ô∏è Rate limit reached for model", model_name)
            return None, "rate_limit"
        else:
            raise

    result = resp.choices[0].message.content
    usage = getattr(resp, "usage", None)
    return result, usage

# –ü—Ä–∏–º–µ—Ä –ª–æ–≥–∏–∫–∏ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
def ask(prompt):
    # –º–æ–¥–µ–ª—å –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
    for m in models:
        ans, usage_or_flag = chat_with_model_auto(m, prompt, site_url="", site_name="")
        if ans is not None:
            return ans
        # –µ—Å–ª–∏ rate_limit, –ø—Ä–æ–±—É–µ–º –¥–∞–ª—å—à–µ
    # –µ—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ ‚Äî –æ—à–∏–±–∫–∞
    raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –º–æ–¥–µ–ª—å —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º –ª–∏–º–∏—Ç–æ–º")


def pdf_to_html(source_dir="."):
    """Convert PDF files to HTML using unstructured library with improved structure for command documentation"""
    docs = os.listdir(source_dir)
    for doc in docs:
        if not doc.endswith('.pdf'):
            continue
        
        html_filename = doc.replace('.pdf', '.html')
        if html_filename in docs:
            print(f"‚è≠Ô∏è  Skipping {doc} - HTML already exists")
            continue
        
        try:
            doc_path = os.path.join(source_dir, doc)
            print(f"üìÑ Processing {doc}...")
            
            # Partition the PDF file
            elements = partition(filename=doc_path)
            
            # Convert elements to HTML with h1-only structure
            html_parts = []
            current_section = None
            section_content = []
            
            # Add CSS for better structure
            html_parts.append("""
<style>
    .section { margin: 20px 0; }
    .section-content { margin: 10px 0; padding: 10px; border: 1px solid #ddd; }
</style>
""")
            
            for element in elements:
                element_type = type(element).__name__
                text = str(element).strip()
                
                if not text:
                    continue
                
                # Check if this is a section heading (numbered, like "6. –ò–ù–¢–ï–†–§–ï–ô–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø")
                if element_type == "Title" and text[0].isdigit() and '. ' in text[:10]:
                    # Save previous section if exists
                    if current_section and section_content:
                        content_html = "".join(section_content)
                        html_parts.append(f'<div class="section"><h1>{current_section}</h1><div class="section-content">{content_html}</div></div>')
                        section_content = []
                    
                    current_section = text
                else:
                    # Add all other content to the current section
                    section_content.append(f"<p>{text}</p>")
            
            # Save last section if exists
            if current_section and section_content:
                content_html = "".join(section_content)
                html_parts.append(f'<div class="section"><h1>{current_section}</h1><div class="section-content">{content_html}</div></div>')
            
            html_str = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>{doc}</title>
</head>
<body>
{''.join(html_parts)}
</body>
</html>"""
            
            output_path = os.path.join(source_dir, html_filename)
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(html_str)
            
            print(f"‚úÖ Converted {doc} to {html_filename}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error processing {doc}: {e}")



if __name__ == "__main__":
    # 1. –°–∫–∞—á–∏–≤–∞–µ–º, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    download_docs_if_needed()

    # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF –≤ HTML (–µ—Å–ª–∏ –µ—Å—Ç—å PDF –≤ supertel_docs)
    if os.path.exists(supertel_dir):
        pdf_to_html(supertel_dir)
    
    # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —á–∞–Ω–∫—É–µ–º –∏–∑ –æ–±–µ–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    chunks = load_all_chunks()
    if not chunks:
        raise RuntimeError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞.")
    
    print(f"üìä –í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
    
    # 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ Qdrant
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

    # 5. –°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    ensure_collection_exists(client, model, chunks)

    # 6. –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞
    # question = 'create l3vpn with on vrfs TEST1 with rd 1234:123 on router 1.1.1.1 and router 2.2.2.2. Give me 2 configs to this routers'
    question = '–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è supertel nms –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ snmp'

    bm25_res = query_bm25(question, 3)
    # print("bm25_res:",bm25_res)
    with open("bm25_res.txt", "w", encoding="utf-8") as f:
        f.write("".join(bm25_res))
    vector_res = query_vector(question, 3)
    with open("vector_res.txt", "w", encoding="utf-8") as f:
        f.write("".join(vector_res))
    # print("vector_res:",vector_res)

    seen = set()
    hybrid = []
    for t in bm25_res:
        if t not in seen:
            hybrid.append(t); seen.add(t)
    for t in vector_res:
        if t not in seen and len(hybrid) < 6:
            hybrid.append(t); seen.add(t)

    context = "\n\n---\n\n".join(hybrid)

    prompt = f"""
You are a helpful assistant for network engineer that work with communication equipment. 
Answer the question using the context below.
If answer is not found, say 'not enough data'.
Answer briefly and to the point. Answer questions primarily using configuration commands, writing them down in ```.
Question: {question}

Context:
{context}
"""

    with open("prompt.txt", "w", encoding="utf-8") as f:
        f.write(prompt)

    print("‚úÖ –ü—Ä–æ–º–ø—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ prompt.txt")
    output = chat_with_model(models[2],prompt)
    print(output)